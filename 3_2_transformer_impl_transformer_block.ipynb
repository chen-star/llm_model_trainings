{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXdTq9TF8y70SCKMvFUBLo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chen-star/llm_model_trainings/blob/main/3_2_transformer_impl_transformer_block.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> â­ Transformer Block â­"
      ],
      "metadata": {
        "id": "CyTOanowsnP5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34a710d"
      },
      "source": [
        "This notebook builds and demonstrates a single Transformer Block using PyTorch.\n",
        "\n",
        "*  **AttentionHead Class**: A custom `AttentionHead` module is implemented, which calculates scaled dot-product attention with `is_causal=True`.\n",
        "\n",
        "*  **MLP Class**: A `MLP` (Multi-Layer Perceptron) module is created, incorporating two linear layers and a GELU activation function.\n",
        "\n",
        "*  **TransformerBlock Class**: The `TransformerBlock` module combines the `AttentionHead` and `MLP` components. It includes Layer Normalization before each sub-layer (attention and MLP) and applies residual connections, following the standard Transformer architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœˆ Imports"
      ],
      "metadata": {
        "id": "wWZLuOlNh6WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from typing import override"
      ],
      "metadata": {
        "id": "5irE2ifbh_iy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¢ Hyperparameters"
      ],
      "metadata": {
        "id": "aPO47hwNiHF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3\n",
        "embedding_dimension = 13\n",
        "context_window_size = 8"
      ],
      "metadata": {
        "id": "wUSSg23aiAm_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] ðŸ‘“ Single Attention Head"
      ],
      "metadata": {
        "id": "d-z4VC3Ficdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedding_dimension):\n",
        "    super().__init__()\n",
        "\n",
        "    # define W_Q, W_K, W_V\n",
        "    self.q_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "    self.k_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "    self.v_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "    # define W0\n",
        "    self.w0_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X):\n",
        "    # Q = XW_Q\n",
        "    # K = XW_K\n",
        "    # V = XW_V\n",
        "    Q = self.q_layer(X)\n",
        "    K = self.k_layer(X)\n",
        "    V = self.v_layer(X)\n",
        "\n",
        "    attention_score = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
        "    return self.w0_layer(attention_score)"
      ],
      "metadata": {
        "id": "o5FACXdLia3w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] ðŸƒ Single MLP"
      ],
      "metadata": {
        "id": "CIr9sakEn85i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, embedding_dimension, expansion: int=4):\n",
        "    super().__init__()\n",
        "\n",
        "    # define W1, Gelu, W2\n",
        "    self.w1_layer = nn.Linear(embedding_dimension, expansion * embedding_dimension) # 4x expansion\n",
        "    self.gelu = nn.GELU()\n",
        "    self.w2_layer = nn.Linear(expansion * embedding_dimension, embedding_dimension) # 4x contraction\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X):\n",
        "    W1 = self.w1_layer(X)\n",
        "    GELU = self.gelu(W1)\n",
        "    W2 = self.w2_layer(GELU)\n",
        "\n",
        "    return W2"
      ],
      "metadata": {
        "id": "XcoDgtubn3Xu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [3] ðŸ”² Single Transformer Block"
      ],
      "metadata": {
        "id": "oH1BiDHOphLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embedding_dimension):\n",
        "    super().__init__()\n",
        "\n",
        "    # Attention\n",
        "    self.layerNorm_attention = nn.LayerNorm(embedding_dimension)\n",
        "    self.attention_head = AttentionHead(embedding_dimension)\n",
        "\n",
        "    # MLP / FeedForward\n",
        "    self.layerNorm_mlp = nn.LayerNorm(embedding_dimension)\n",
        "    self.mlp = MLP(embedding_dimension)\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X):\n",
        "    # --- Attention ---\n",
        "    # X -> layerNorm -> attention_head\n",
        "    #                                     +   = output\n",
        "    #                                X\n",
        "    X = X + self.attention_head(self.layerNorm_attention(X))\n",
        "\n",
        "    # --- MLP ---\n",
        "    # X -> layerNorm -> mlp\n",
        "    #                         +   = output\n",
        "    #                     X\n",
        "    X = X + self.mlp(self.layerNorm_mlp(X))\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "vDL8xyMrpdGz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_block = TransformerBlock(embedding_dimension)\n",
        "print(transformer_block)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks41tfioriOB",
        "outputId": "1da2b149-e701-435e-a18f-087b21e8ccff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerBlock(\n",
            "  (layerNorm_attention): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n",
            "  (attention_head): AttentionHead(\n",
            "    (q_layer): Linear(in_features=13, out_features=13, bias=False)\n",
            "    (k_layer): Linear(in_features=13, out_features=13, bias=False)\n",
            "    (v_layer): Linear(in_features=13, out_features=13, bias=False)\n",
            "    (w0_layer): Linear(in_features=13, out_features=13, bias=False)\n",
            "  )\n",
            "  (layerNorm_mlp): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n",
            "  (mlp): MLP(\n",
            "    (w1_layer): Linear(in_features=13, out_features=52, bias=True)\n",
            "    (gelu): GELU(approximate='none')\n",
            "    (w2_layer): Linear(in_features=52, out_features=13, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pass data once to test\n",
        "random_X = torch.randn(batch_size, context_window_size, embedding_dimension)\n",
        "output = transformer_block(random_X)\n",
        "print(f\"Output shape: \\n{output.shape}\\n\")\n",
        "print(f\"Output:  \\n{output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWkzd22Krmvk",
        "outputId": "39ef363a-6f79-4d50-8d01-d987d8cc9c12"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: \n",
            "torch.Size([3, 8, 13])\n",
            "\n",
            "Output:  \n",
            "tensor([[[-6.7200e-01, -1.0021e+00,  6.4806e-02,  6.9468e-01, -2.0332e+00,\n",
            "          -1.7624e-01, -5.5038e-01, -1.0213e+00, -1.8001e+00,  4.8956e-01,\n",
            "          -2.1546e-01,  1.0365e+00, -1.4063e+00],\n",
            "         [-5.3165e-01,  1.1005e+00,  3.9205e-01,  6.1127e-01, -1.4813e+00,\n",
            "           1.3676e+00,  2.0045e+00,  6.4204e-01, -1.9562e-03, -9.0771e-01,\n",
            "           2.1294e-01,  1.7055e+00, -3.5207e+00],\n",
            "         [-1.6778e-01, -1.1007e+00,  7.3168e-02, -2.0428e-02,  6.6337e-01,\n",
            "           2.2267e+00, -1.2982e+00,  3.3505e-01,  2.4017e+00,  3.0655e-02,\n",
            "          -9.4797e-01, -1.3617e+00,  4.6341e-01],\n",
            "         [-4.3264e-01,  1.5660e+00,  4.2658e-01, -1.1551e+00, -2.0015e-01,\n",
            "           7.7103e-02,  5.1251e-01, -1.8463e+00,  5.8794e-02,  4.1444e-01,\n",
            "           2.3806e-01, -8.0845e-01, -2.3914e+00],\n",
            "         [ 1.5072e+00,  1.0973e+00, -2.8834e-01,  4.2662e-02,  1.6436e-01,\n",
            "           3.1724e-02,  2.8492e-01, -1.1419e+00, -2.8594e-01,  6.2059e-01,\n",
            "           4.6164e-01,  3.1707e-02, -3.6701e-01],\n",
            "         [-2.7676e-01, -2.1419e+00,  1.8979e+00,  8.4745e-01, -3.3275e-01,\n",
            "          -3.8215e-01, -9.0423e-01,  9.5609e-01,  9.5118e-01, -3.4727e-01,\n",
            "          -1.2681e+00,  1.0687e+00, -1.4695e+00],\n",
            "         [ 8.7701e-01, -1.6511e+00, -1.9047e+00, -1.8576e+00, -9.9354e-01,\n",
            "           2.7661e-02,  2.5789e-01,  1.5357e+00, -7.2771e-01,  1.5954e+00,\n",
            "          -9.0202e-01, -9.1428e-01, -1.0118e-01],\n",
            "         [ 1.4569e+00,  3.7484e-01,  8.2629e-02,  4.0532e-01, -1.3170e+00,\n",
            "          -3.0834e-01,  6.8277e-01, -7.8200e-01,  1.8421e+00,  1.9197e+00,\n",
            "          -1.5301e-01, -1.6261e+00, -8.0052e-01]],\n",
            "\n",
            "        [[-1.2000e+00, -5.1436e-01,  8.7101e-01,  1.1180e+00, -1.1201e+00,\n",
            "           3.3399e-01, -5.2793e-01,  3.5299e-01, -7.5563e-01,  1.2418e+00,\n",
            "          -1.1671e+00,  3.8539e-01,  5.8560e-01],\n",
            "         [-8.3101e-01, -3.1754e-01, -1.2236e+00, -4.3067e-01,  1.2971e+00,\n",
            "          -1.0346e-01, -6.2849e-01, -4.5116e-01, -3.6288e-01,  2.9242e-01,\n",
            "          -2.8354e-01, -7.3150e-01, -1.6785e+00],\n",
            "         [ 4.0855e-01, -1.4268e+00,  5.9424e-02, -1.0062e+00, -1.1294e+00,\n",
            "           1.2188e+00, -1.1936e+00, -7.8937e-01, -8.2065e-01,  1.5193e-01,\n",
            "          -1.5746e+00, -1.3084e+00,  2.5936e-01],\n",
            "         [ 4.3562e-01, -7.9472e-01,  1.0177e+00, -3.1505e-01, -4.0062e-01,\n",
            "          -7.7162e-01,  1.9914e+00,  1.1090e+00,  5.2980e-01,  6.9933e-01,\n",
            "           2.0495e+00, -3.7894e-01,  1.6068e-01],\n",
            "         [-2.8309e-01,  1.6117e-01, -7.3340e-01, -7.9348e-01,  2.1585e+00,\n",
            "           9.2745e-01, -6.8902e-01, -2.6062e-01,  1.3649e+00, -1.2421e+00,\n",
            "           1.0976e+00,  6.5732e-01,  2.8342e-01],\n",
            "         [ 2.2279e+00,  8.4735e-01, -1.4760e-01, -2.9039e-01, -2.3497e-01,\n",
            "           6.2355e-01,  6.8540e-01, -1.0699e+00,  3.1245e-01, -1.1830e+00,\n",
            "           1.1402e+00, -1.0376e+00,  9.9995e-01],\n",
            "         [-5.7369e-01, -7.0233e-01,  5.3627e-01,  6.0451e-01,  1.2776e+00,\n",
            "           1.2065e+00, -4.2337e-01, -5.5863e-01,  1.2018e+00, -1.3320e+00,\n",
            "           9.4805e-01,  1.1879e-01,  5.7051e-01],\n",
            "         [-9.5396e-01, -1.2240e+00,  1.8529e+00,  9.5738e-01, -1.1401e+00,\n",
            "           2.0896e+00,  1.4470e+00, -2.0214e-01, -1.4266e+00,  3.1601e-01,\n",
            "          -1.1743e+00,  1.5635e+00,  1.4209e-01]],\n",
            "\n",
            "        [[-2.0511e-01,  6.2105e-01, -1.7157e-01,  2.1266e-01, -5.8665e-01,\n",
            "          -2.7830e-01, -3.0389e-01,  6.6313e-02,  6.8820e-01,  7.6101e-01,\n",
            "          -1.3535e+00, -4.1229e-01,  3.7388e-01],\n",
            "         [ 7.5781e-01,  1.5649e-01,  9.2059e-01, -1.3701e+00, -1.6993e-01,\n",
            "           1.5822e+00,  1.6225e-01, -2.4296e-01,  3.9738e-01, -1.5250e+00,\n",
            "           1.0709e+00, -7.8208e-01,  8.1956e-01],\n",
            "         [ 8.4898e-02, -9.9925e-02,  2.3475e-01,  9.7593e-01,  4.8230e-01,\n",
            "           1.0832e+00,  1.3735e-01, -2.7217e+00, -7.5360e-01,  7.7306e-01,\n",
            "          -1.9501e-01, -2.5963e-01,  1.0289e+00],\n",
            "         [ 1.2660e+00,  1.4279e+00, -1.0832e+00,  4.7230e-01,  3.7255e-01,\n",
            "          -1.5533e-01,  1.1710e+00, -1.3392e+00,  2.5798e-01, -2.2820e-01,\n",
            "           1.0229e+00, -8.9281e-01,  3.1346e-01],\n",
            "         [ 9.9643e-01,  1.6925e-01,  8.9935e-01,  8.1861e-01, -1.0513e+00,\n",
            "          -1.8116e+00,  1.9436e-01, -3.1721e-01, -4.7340e-02, -8.5184e-01,\n",
            "          -1.0752e-01,  1.4716e+00, -1.3332e-01],\n",
            "         [ 5.9740e-01,  6.7324e-01,  4.5103e-01, -1.0117e+00,  1.2028e+00,\n",
            "           1.2703e+00, -6.0421e-01, -1.0536e+00,  6.8967e-01,  7.5172e-01,\n",
            "           8.4388e-01, -2.7202e-01,  6.8672e-01],\n",
            "         [ 1.2659e+00, -3.8611e-01, -4.9835e-01, -6.0627e-01,  8.7201e-01,\n",
            "           1.6499e+00,  1.1785e+00, -3.0454e-02, -8.7089e-01,  1.1046e+00,\n",
            "           3.2661e-01,  1.1084e-02,  6.3836e-01],\n",
            "         [ 8.8694e-01,  1.3438e+00,  2.1017e-01,  5.8665e-01,  1.7468e-01,\n",
            "          -9.8676e-01,  1.1853e+00, -9.4856e-01, -2.0371e+00,  1.9512e+00,\n",
            "           2.1007e-01, -5.6651e-02, -2.4544e-01]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}