{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/hsXv2LhxIoB88blogsOz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chen-star/llm_model_trainings/blob/main/3_3_transformer_impl_multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> ‚≠ê Multi-head Attention ‚≠ê"
      ],
      "metadata": {
        "id": "RlpHWrpY4h30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úà Imports"
      ],
      "metadata": {
        "id": "gcCYqwXo4n_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from typing import override"
      ],
      "metadata": {
        "id": "KT5BtxTb4l9m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî¢ Hyperparameters"
      ],
      "metadata": {
        "id": "kXZKjHjA4uQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3\n",
        "\n",
        "embedding_dimension = 100\n",
        "num_heads = 4 # embedding_dimension must be divisible by num_heads\n",
        "\n",
        "context_window_size = 8"
      ],
      "metadata": {
        "id": "NY_5gEHr4sCC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üëì Multi-head Attention"
      ],
      "metadata": {
        "id": "6SmgMo_c5KPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review the single-head attention defined in [3_2_transformer_impl_transformer_block.ipynb](https://github.com/chen-star/llm_model_trainings/blob/main/3_2_transformer_impl_transformer_block.ipynb)"
      ],
      "metadata": {
        "id": "SSYPxhAB5cLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedding_dimension):\n",
        "    super().__init__()\n",
        "\n",
        "    # define W_Q, W_K, W_V\n",
        "    self.q_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "    self.k_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "    self.v_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "    # define W0\n",
        "    self.w0_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X):\n",
        "    # Q = XW_Q\n",
        "    # K = XW_K\n",
        "    # V = XW_V\n",
        "    Q = self.q_layer(X)\n",
        "    K = self.k_layer(X)\n",
        "    V = self.v_layer(X)\n",
        "\n",
        "    attention_score = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
        "    return self.w0_layer(attention_score)"
      ],
      "metadata": {
        "id": "DuUypacg5a_b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the multi-head attention"
      ],
      "metadata": {
        "id": "LVfe2d1U5v6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedding_dimension, num_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    # define W_Q, W_K, W_V\n",
        "    self.q_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "    self.k_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "    self.v_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "    # define W0\n",
        "    self.w0_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "    # ***** New in multi-head *****\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dimension = embedding_dimension // num_heads\n",
        "    # *****************************\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X, print_dimension_info: bool=False):\n",
        "    batch_size, context_window_size, embedding_dimension = X.shape\n",
        "    if print_dimension_info: print(f\"X.shape: {X.shape}\")\n",
        "\n",
        "    # Q = XW_Q\n",
        "    # K = XW_K\n",
        "    # V = XW_V\n",
        "    Q = self.q_layer(X)\n",
        "    K = self.k_layer(X)\n",
        "    V = self.v_layer(X)\n",
        "    if print_dimension_info: print(f\"Before split: Q.shape: {Q.shape}, K.shape: {K.shape}, V.shape: {V.shape}\")\n",
        "\n",
        "    # ***** Split Q,K,V *****\n",
        "    Q = Q.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
        "    K = K.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
        "    V = V.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
        "    if print_dimension_info: print(f\"After split: Q.shape: {Q.shape}, K.shape: {K.shape}, V.shape: {V.shape}\")\n",
        "\n",
        "    # For attention score calculation, pytorch expects the shape to be\n",
        "    # [batch_size, num_heads, context_window_size, head_dimension]\n",
        "    Q = Q.transpose(1,2)\n",
        "    K = K.transpose(1,2)\n",
        "    V = V.transpose(1,2)\n",
        "    if print_dimension_info: print(f\"After transpose: Q.shape: {Q.shape}, K.shape: {K.shape}, V.shape: {V.shape}\")\n",
        "    # *****************************\n",
        "\n",
        "    attention_score = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
        "    if print_dimension_info: print(f\"attention_score.shape: {attention_score.shape}\")\n",
        "\n",
        "    # Transpose back\n",
        "    attention_score = attention_score.transpose(1,2)\n",
        "    if print_dimension_info: print(f\"Transposed attention_score.shape: {attention_score.shape}\")\n",
        "\n",
        "    # ***** Merge heads *****\n",
        "    attention_score = attention_score.reshape(batch_size, context_window_size, embedding_dimension)\n",
        "    if print_dimension_info: print(f\"After merge heads: attention_score.shape: {attention_score.shape}\")\n",
        "    # *****************************\n",
        "\n",
        "    return self.w0_layer(attention_score)"
      ],
      "metadata": {
        "id": "F44ZDtL35EnB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_head_attention = MultiHeadAttention(embedding_dimension, num_heads=num_heads)\n",
        "multi_head_attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U_GtdFy6AC2",
        "outputId": "804e57f7-3252-4554-a551-52402820b8cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiHeadAttention(\n",
              "  (q_layer): Linear(in_features=100, out_features=100, bias=False)\n",
              "  (k_layer): Linear(in_features=100, out_features=100, bias=False)\n",
              "  (v_layer): Linear(in_features=100, out_features=100, bias=False)\n",
              "  (w0_layer): Linear(in_features=100, out_features=100, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pass data once to test\n",
        "print(f\"batch_size: {batch_size}\")\n",
        "print(f\"context_window_size: {context_window_size}\")\n",
        "print(f\"embedding_dimension: {embedding_dimension}\")\n",
        "print(f\"num_heads: {num_heads}\")\n",
        "print(f\"head_dimension: {embedding_dimension // num_heads}\\n\")\n",
        "\n",
        "random_X = torch.randn(batch_size, context_window_size, embedding_dimension)\n",
        "output = multi_head_attention(random_X, print_dimension_info=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHrjTYNQ9KSZ",
        "outputId": "0c87ac2d-abab-4d3c-ac97-3ad5db4fff07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size: 3\n",
            "context_window_size: 8\n",
            "embedding_dimension: 100\n",
            "num_heads: 4\n",
            "head_dimension: 25\n",
            "\n",
            "X.shape: torch.Size([3, 8, 100])\n",
            "Before split: Q.shape: torch.Size([3, 8, 100]), K.shape: torch.Size([3, 8, 100]), V.shape: torch.Size([3, 8, 100])\n",
            "After split: Q.shape: torch.Size([3, 8, 4, 25]), K.shape: torch.Size([3, 8, 4, 25]), V.shape: torch.Size([3, 8, 4, 25])\n",
            "After transpose: Q.shape: torch.Size([3, 4, 8, 25]), K.shape: torch.Size([3, 4, 8, 25]), V.shape: torch.Size([3, 4, 8, 25])\n",
            "attention_score.shape: torch.Size([3, 4, 8, 25])\n",
            "Transposed attention_score.shape: torch.Size([3, 8, 4, 25])\n",
            "After merge heads: attention_score.shape: torch.Size([3, 8, 100])\n"
          ]
        }
      ]
    }
  ]
}