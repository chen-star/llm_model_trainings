{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNx6azdmXRum77E71HTT1AE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chen-star/llm_model_trainings/blob/main/3_4_transformer_impl_full_transformer_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> ‚≠ê Transformer Decoder ‚≠ê"
      ],
      "metadata": {
        "id": "PivjYl26e5J2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úà Imports"
      ],
      "metadata": {
        "id": "EVpFJ9BSe_F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import override"
      ],
      "metadata": {
        "id": "soTikDf3e3zN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî¢ Hyperparameters"
      ],
      "metadata": {
        "id": "KqU2oW6rfOxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv7mbrv-fCub",
        "outputId": "cd2a5069-d235-4bf9-d6b2-f4a2e73d2b03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same parameters a GPT2-124M\n",
        "batch_size = 8\n",
        "\n",
        "num_transformer_blocks = 12\n",
        "\n",
        "embedding_dimension = 768\n",
        "num_heads = 12 # embedding_dimension must be divisible by num_heads\n",
        "\n",
        "context_window_size = 1024\n",
        "vocabulary_size = 50257"
      ],
      "metadata": {
        "id": "K9jfxFGJfR3g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] üèö Model Impl"
      ],
      "metadata": {
        "id": "LTPgxq3uf3wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1.1) üëì Multi-head Attention"
      ],
      "metadata": {
        "id": "A581doZHgD08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedding_dimension, num_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    # define W_Q, W_K, W_V\n",
        "    self.q_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "    self.k_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "    self.v_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "    # define W0\n",
        "    self.w0_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "    # ***** multi-head *****\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dimension = embedding_dimension // num_heads\n",
        "    # *****************************\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X):\n",
        "    batch_size, context_window_size, embedding_dimension = X.shape\n",
        "\n",
        "    # Q = XW_Q\n",
        "    # K = XW_K\n",
        "    # V = XW_V\n",
        "    Q = self.q_layer(X)\n",
        "    K = self.k_layer(X)\n",
        "    V = self.v_layer(X)\n",
        "\n",
        "    # ***** Split Q,K,V *****\n",
        "    Q = Q.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
        "    K = K.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
        "    V = V.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
        "\n",
        "    # For attention score calculation, pytorch expects the shape to be\n",
        "    # [batch_size, num_heads, context_window_size, head_dimension]\n",
        "    Q = Q.transpose(1,2)\n",
        "    K = K.transpose(1,2)\n",
        "    V = V.transpose(1,2)\n",
        "    # *****************************\n",
        "\n",
        "    attention_score = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
        "\n",
        "    # Transpose back\n",
        "    attention_score = attention_score.transpose(1,2)\n",
        "\n",
        "    # ***** Merge heads *****\n",
        "    attention_score = attention_score.reshape(batch_size, context_window_size, embedding_dimension)\n",
        "    # *****************************\n",
        "\n",
        "    return self.w0_layer(attention_score)"
      ],
      "metadata": {
        "id": "dpJ3YTAzf0GC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1.2) üèÉ Single MLP"
      ],
      "metadata": {
        "id": "UwrxJQmnkmKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, embedding_dimension, expansion: int=4):\n",
        "    super().__init__()\n",
        "\n",
        "    # define W1, Gelu, W2\n",
        "    self.w1_layer = nn.Linear(embedding_dimension, expansion * embedding_dimension) # 4x expansion\n",
        "    self.gelu = nn.GELU()\n",
        "    self.w2_layer = nn.Linear(expansion * embedding_dimension, embedding_dimension) # 4x contraction\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X):\n",
        "    W1 = self.w1_layer(X)\n",
        "    GELU = self.gelu(W1)\n",
        "    W2 = self.w2_layer(GELU)\n",
        "\n",
        "    return W2"
      ],
      "metadata": {
        "id": "xk4TGIuRksF3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1.3) üî≤ Transformer Block"
      ],
      "metadata": {
        "id": "bEfV1ZcGkTEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embedding_dimension):\n",
        "    super().__init__()\n",
        "\n",
        "    # Attention\n",
        "    self.layerNorm_attention = nn.LayerNorm(embedding_dimension)\n",
        "    self.attention_heads = MultiHeadAttention(embedding_dimension, num_heads)\n",
        "\n",
        "    # MLP / FeedForward\n",
        "    self.layerNorm_mlp = nn.LayerNorm(embedding_dimension)\n",
        "    self.mlp = MLP(embedding_dimension)\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X):\n",
        "    # --- Attention ---\n",
        "    # X -> layerNorm -> attention_head\n",
        "    #                                     +   = output\n",
        "    #                                X\n",
        "    X = X + self.attention_heads(self.layerNorm_attention(X))\n",
        "\n",
        "    # --- MLP ---\n",
        "    # X -> layerNorm -> mlp\n",
        "    #                         +   = output\n",
        "    #                     X\n",
        "    X = X + self.mlp(self.layerNorm_mlp(X))\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "LBZ_wI4pkY6U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1.4) üè¢ Model"
      ],
      "metadata": {
        "id": "5BTpcTT0k-rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # ----- Token Embedding + Position Encoding -----\n",
        "    self.wte = nn.Embedding(vocabulary_size, embedding_dimension) # token embedding\n",
        "    self.wpe = nn.Embedding(context_window_size, embedding_dimension) # position encoding\n",
        "\n",
        "    # ----- Transformer Blocks -----\n",
        "    self.transformer_blocks = nn.Sequential(*[\n",
        "          TransformerBlock(embedding_dimension) for _ in range(num_transformer_blocks)\n",
        "        ])\n",
        "\n",
        "    # ----- Final layernorm -----\n",
        "    self.final_layernorm = nn.LayerNorm(embedding_dimension)\n",
        "\n",
        "    # ----- Unembedding -----\n",
        "    self.unembedding = nn.Linear(embedding_dimension, vocabulary_size, bias=False)\n",
        "    # tied unembedding weights\n",
        "    self.unembedding.weight = nn.Parameter(self.wte.weight)\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, token_ids):\n",
        "    # ----- Token Embedding + Position Encoding -----\n",
        "    # [batch_size, context_window_size, embedding_dimension]\n",
        "    token_embedding = self.wte(token_ids)\n",
        "    # [context_window_size, embedding_dimension]\n",
        "    position_emcoding = self.wpe(torch.arange(token_ids.shape[-1], device=device))\n",
        "    # [batch_size, context_window_size, embedding_dimension]\n",
        "    X = token_embedding + position_emcoding\n",
        "\n",
        "    # ----- Transformer Blocks -----\n",
        "    X = self.transformer_blocks(X)\n",
        "\n",
        "    # ----- Final layernorm -----\n",
        "    X = self.final_layernorm(X)\n",
        "\n",
        "    # ----- Unembedding -----\n",
        "    # [batch_size, context_window_size, vocab_size]\n",
        "    logits = self.unembedding(X)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "  def generate(self, token_ids, temperature=1.1, num_new_tokens=10):\n",
        "    for _ in range(0, num_new_tokens):\n",
        "      # forward\n",
        "      # [batch_size, context_window_size, vocab_size]\n",
        "      logits = self(token_ids[:, -context_window_size:])\n",
        "      # [batch_size, vocab_size]\n",
        "      logits = logits[:, -1, :] # last token's logits\n",
        "\n",
        "      # softmax\n",
        "      # [batch_size, vocab_size]\n",
        "      probabilities = F.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "      # sample\n",
        "      # [batch_size, 1]\n",
        "      next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
        "\n",
        "      # append\n",
        "      token_ids = torch.cat((token_ids, next_token_id), dim=1)\n",
        "\n",
        "      return token_ids"
      ],
      "metadata": {
        "id": "ANWEfeX7k8RO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1.5) üß™ Random Data Test"
      ],
      "metadata": {
        "id": "3wCB9CCCqTB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel().to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oelwbiT3qPG9",
        "outputId": "6974954b-6f0b-4f61-eb37-2b48840a8fb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (wte): Embedding(50257, 768)\n",
              "  (wpe): Embedding(1024, 768)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention_heads): MultiHeadAttention(\n",
              "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "      )\n",
              "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (gelu): GELU(approximate='none')\n",
              "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (unembedding): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pass data once to test\n",
        "print(f\"batch_size: {batch_size}\")\n",
        "print(f\"context_window_size: {context_window_size}\")\n",
        "print(f\"embedding_dimension: {embedding_dimension}\")\n",
        "print(f\"num_heads: {num_heads}\")\n",
        "print(f\"head_dimension: {embedding_dimension // num_heads}\\n\")\n",
        "\n",
        "random_token_ids = torch.randint(0, vocabulary_size, (batch_size, context_window_size)).to(device)\n",
        "output = model(random_token_ids)\n",
        "print(f\"Input shape: {random_token_ids.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tE2RgG7rwDJ",
        "outputId": "9aa81cdb-3fc1-4dd1-985a-2a620b43ae5f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size: 8\n",
            "context_window_size: 1024\n",
            "embedding_dimension: 768\n",
            "num_heads: 12\n",
            "head_dimension: 64\n",
            "\n",
            "Input shape: torch.Size([8, 1024])\n",
            "Output shape: torch.Size([8, 1024, 50257])\n"
          ]
        }
      ]
    }
  ]
}