{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNJi2ZU49j2KSvktmheql6h",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/chen-star/llm_model_trainings/blob/main/3_4_transformer_impl_full_transformer_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> \u2b50 Transformer Decoder \u2b50"
   ],
   "metadata": {
    "id": "PivjYl26e5J2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \u2708 Imports"
   ],
   "metadata": {
    "id": "EVpFJ9BSe_F5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import override"
   ],
   "metadata": {
    "id": "soTikDf3e3zN"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\udd22 Hyperparameters"
   ],
   "metadata": {
    "id": "KqU2oW6rfOxr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# use GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hv7mbrv-fCub",
    "outputId": "34f35979-b074-4780-83b1-01184b47d4c0"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Use the same parameters a GPT2-124M\n",
    "batch_size = 8\n",
    "\n",
    "num_transformer_blocks = 12\n",
    "\n",
    "embedding_dimension = 768\n",
    "num_heads = 12 # embedding_dimension must be divisible by num_heads\n",
    "\n",
    "context_window_size = 1024\n",
    "vocabulary_size = 50257"
   ],
   "metadata": {
    "id": "K9jfxFGJfR3g"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [1] \ud83c\udfda Model Impl"
   ],
   "metadata": {
    "id": "LTPgxq3uf3wi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1.1) \ud83d\udc53 Multi-head Attention"
   ],
   "metadata": {
    "id": "A581doZHgD08"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, embedding_dimension, num_heads):\n",
    "    super().__init__()\n",
    "\n",
    "    # define W_Q, W_K, W_V\n",
    "    self.q_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
    "    self.k_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
    "    self.v_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
    "\n",
    "    # define W0\n",
    "    self.w0_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
    "\n",
    "    # ***** multi-head *****\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dimension = embedding_dimension // num_heads\n",
    "    # *****************************\n",
    "\n",
    "\n",
    "  @override\n",
    "  def forward(self, X):\n",
    "    batch_size, context_window_size, embedding_dimension = X.shape\n",
    "\n",
    "    # Q = XW_Q\n",
    "    # K = XW_K\n",
    "    # V = XW_V\n",
    "    Q = self.q_layer(X)\n",
    "    K = self.k_layer(X)\n",
    "    V = self.v_layer(X)\n",
    "\n",
    "    # ***** Split Q,K,V *****\n",
    "    Q = Q.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
    "    K = K.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
    "    V = V.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
    "\n",
    "    # For attention score calculation, pytorch expects the shape to be\n",
    "    # [batch_size, num_heads, context_window_size, head_dimension]\n",
    "    Q = Q.transpose(1,2)\n",
    "    K = K.transpose(1,2)\n",
    "    V = V.transpose(1,2)\n",
    "    # *****************************\n",
    "\n",
    "    attention_score = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "\n",
    "    # Transpose back\n",
    "    attention_score = attention_score.transpose(1,2)\n",
    "\n",
    "    # ***** Merge heads *****\n",
    "    attention_score = attention_score.reshape(batch_size, context_window_size, embedding_dimension)\n",
    "    # *****************************\n",
    "\n",
    "    return self.w0_layer(attention_score)"
   ],
   "metadata": {
    "id": "dpJ3YTAzf0GC"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1.2) \ud83c\udfc3 Single MLP"
   ],
   "metadata": {
    "id": "UwrxJQmnkmKp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, embedding_dimension, expansion: int=4):\n",
    "    super().__init__()\n",
    "\n",
    "    # define W1, Gelu, W2\n",
    "    self.w1_layer = nn.Linear(embedding_dimension, expansion * embedding_dimension) # 4x expansion\n",
    "    self.gelu = nn.GELU()\n",
    "    self.w2_layer = nn.Linear(expansion * embedding_dimension, embedding_dimension) # 4x contraction\n",
    "\n",
    "\n",
    "  @override\n",
    "  def forward(self, X):\n",
    "    W1 = self.w1_layer(X)\n",
    "    GELU = self.gelu(W1)\n",
    "    W2 = self.w2_layer(GELU)\n",
    "\n",
    "    return W2"
   ],
   "metadata": {
    "id": "xk4TGIuRksF3"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1.3) \ud83d\udd32 Transformer Block"
   ],
   "metadata": {
    "id": "bEfV1ZcGkTEG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, embedding_dimension):\n",
    "    super().__init__()\n",
    "\n",
    "    # Attention\n",
    "    self.layerNorm_attention = nn.LayerNorm(embedding_dimension)\n",
    "    self.attention_heads = MultiHeadAttention(embedding_dimension, num_heads)\n",
    "\n",
    "    # MLP / FeedForward\n",
    "    self.layerNorm_mlp = nn.LayerNorm(embedding_dimension)\n",
    "    self.mlp = MLP(embedding_dimension)\n",
    "\n",
    "\n",
    "  @override\n",
    "  def forward(self, X):\n",
    "    # --- Attention ---\n",
    "    # X -> layerNorm -> attention_head\n",
    "    #                                     +   = output\n",
    "    #                                X\n",
    "    X = X + self.attention_heads(self.layerNorm_attention(X))\n",
    "\n",
    "    # --- MLP ---\n",
    "    # X -> layerNorm -> mlp\n",
    "    #                         +   = output\n",
    "    #                     X\n",
    "    X = X + self.mlp(self.layerNorm_mlp(X))\n",
    "\n",
    "    return X"
   ],
   "metadata": {
    "id": "LBZ_wI4pkY6U"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1.4) \ud83c\udfe2 Model"
   ],
   "metadata": {
    "id": "5BTpcTT0k-rd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LanguageModel(nn.Module):\n",
    "  def __init__(self, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.device = device\n",
    "\n",
    "    # ----- Token Embedding + Position Encoding -----\n",
    "    self.wte = nn.Embedding(vocabulary_size, embedding_dimension) # token embedding\n",
    "    self.wpe = nn.Embedding(context_window_size, embedding_dimension) # position encoding\n",
    "\n",
    "    # ----- Transformer Blocks -----\n",
    "    self.transformer_blocks = nn.Sequential(*[\n",
    "          TransformerBlock(embedding_dimension) for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "\n",
    "    # ----- Final layernorm -----\n",
    "    self.final_layernorm = nn.LayerNorm(embedding_dimension)\n",
    "\n",
    "    # ----- Unembedding -----\n",
    "    self.unembedding = nn.Linear(embedding_dimension, vocabulary_size, bias=False)\n",
    "    # tied unembedding weights\n",
    "    self.unembedding.weight = nn.Parameter(self.wte.weight)\n",
    "\n",
    "\n",
    "  @override\n",
    "  def forward(self, token_ids):\n",
    "    # ----- Token Embedding + Position Encoding -----\n",
    "    # [batch_size, context_window_size, embedding_dimension]\n",
    "    token_embedding = self.wte(token_ids)\n",
    "    # [context_window_size, embedding_dimension]\n",
    "    position_emcoding = self.wpe(torch.arange(token_ids.shape[-1], device=self.device))\n",
    "    # [batch_size, context_window_size, embedding_dimension]\n",
    "    X = token_embedding + position_emcoding\n",
    "\n",
    "    # ----- Transformer Blocks -----\n",
    "    X = self.transformer_blocks(X)\n",
    "\n",
    "    # ----- Final layernorm -----\n",
    "    X = self.final_layernorm(X)\n",
    "\n",
    "    # ----- Unembedding -----\n",
    "    # [batch_size, context_window_size, vocab_size]\n",
    "    logits = self.unembedding(X)\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "  def generate(self, token_ids, temperature=1.1, num_new_tokens=10):\n",
    "    for _ in range(0, num_new_tokens):\n",
    "      # forward\n",
    "      # [batch_size, context_window_size, vocab_size]\n",
    "      logits = self(token_ids[:, -context_window_size:])\n",
    "      # [batch_size, vocab_size]\n",
    "      logits = logits[:, -1, :] # last token's logits\n",
    "\n",
    "      # softmax\n",
    "      # [batch_size, vocab_size]\n",
    "      probabilities = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "      # sample\n",
    "      # [batch_size, 1]\n",
    "      next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "      # append\n",
    "      token_ids = torch.cat((token_ids, next_token_id), dim=1)\n",
    "\n",
    "      return token_ids"
   ],
   "metadata": {
    "id": "ANWEfeX7k8RO"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1.5) \ud83e\uddea Random Data Test"
   ],
   "metadata": {
    "id": "3wCB9CCCqTB-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = LanguageModel(device).to(device)\n",
    "model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oelwbiT3qPG9",
    "outputId": "6980fd2b-e5bf-4b1a-f317-21adcac7eb88"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layerNorm_attention): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention_heads): MultiHeadAttention(\n",
       "        (q_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (v_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w0_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (layerNorm_mlp): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (w1_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (w2_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (unembedding): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# pass data once to test\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "print(f\"context_window_size: {context_window_size}\")\n",
    "print(f\"embedding_dimension: {embedding_dimension}\")\n",
    "print(f\"num_heads: {num_heads}\")\n",
    "print(f\"head_dimension: {embedding_dimension // num_heads}\\n\")\n",
    "\n",
    "random_token_ids = torch.randint(0, vocabulary_size, (batch_size, context_window_size)).to(device)\n",
    "output = model(random_token_ids)\n",
    "print(f\"Input shape: {random_token_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tE2RgG7rwDJ",
    "outputId": "fb4a4fa0-f59d-4731-ab89-4497260afd58"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch_size: 8\n",
      "context_window_size: 1024\n",
      "embedding_dimension: 768\n",
      "num_heads: 12\n",
      "head_dimension: 64\n",
      "\n",
      "Input shape: torch.Size([8, 1024])\n",
      "Output shape: torch.Size([8, 1024, 50257])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [2] \ud83d\uddfa Compare with GPT2 Model"
   ],
   "metadata": {
    "id": "8lfv-2mJw9MQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM,GPT2Tokenizer\n",
    "\n",
    "!pip install torchinfo # not installed by default in colab\n",
    "from torchinfo import summary"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBTsuZs0xFjz",
    "outputId": "3d7783e1-039c-4da7-a3c9-84869526a603"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "2b154b88ba1a4a6d9e160233c8534e23",
      "d1093e0b5cc64127aac828ddf00f2ec7",
      "0aaf44a8ab7c49b1ba2c5f72280ca875",
      "4f50b998d34b446ca5a0cb3b77653296",
      "7d1b0626afdd4b31bf782a60817173e5",
      "9c272530d3a6456896b570ec007631b4",
      "5295035f078a460ab05665cc69cdbe7e",
      "7d9dc8ea5c6b4c94960ed98a59aee68f",
      "f0b3ef85250c4fffa02a963328b4a297",
      "83937869d7fd4a5b863aa3bb79b9aa24",
      "92333d561c464a02bdf9bcf0a61bf9d3",
      "6860c87805fa42769f81ad84eeda54ab",
      "d35571d2d60940fdb5ee1a2340dd2e76",
      "5c6acfb934b64f178cb8593fc89e745e",
      "5acb4349e1884d21bd727a885445f025",
      "ed532cd737344dd883dad140bdda193c",
      "816b3fccd0ff48868435983f0311d65d",
      "737ed73a3ef04aa4a8ba70af1baa1b0e",
      "c7b77cdf528c4a7f97fd01aa9ad08fc5",
      "8f7b3c0d6853458698bc91c551a54066",
      "02a9e34adcbb41c9bc09ab0d8f4adb19",
      "4b8ab278bae649a4a06efb61c4e5492e",
      "5311b7fc0e9a4c789fe47e4e35721d9c",
      "82ba7e4be9ba4ef991b4f4b593a5ebd8",
      "8f10f1b63cfa4a1e9af052971d2572a8",
      "4c8172970b254ff0a2bb5329fde32e4e",
      "2f5fca41f51e44d89e8438326c989a87",
      "0e9a38d3a0674a9e8a6e3df625510b0e",
      "9d382a9c92254c8fb256d1fdb6c052b0",
      "4fd16ccb81fa46f0a696b77b3ac1e6c7",
      "79061009258a412ba5729d88b32e550c",
      "f08b5f502eb34e088386e796bcce387d",
      "3363d74d781247f1a561bcccdaa16fad"
     ]
    },
    "id": "nT9eOCHqxKU-",
    "outputId": "b6b50430-c460-47db-dcc9-48fd28fe8a00"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b154b88ba1a4a6d9e160233c8534e23"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6860c87805fa42769f81ad84eeda54ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5311b7fc0e9a4c789fe47e4e35721d9c"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3dacb8f",
    "outputId": "0c38c954-e7b2-484d-fae9-0cfd99a1abc1"
   },
   "source": [
    "summary(model, input_size=(batch_size, context_window_size), dtypes=[torch.long])"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LanguageModel                            [8, 1024, 50257]          --\n",
       "\u251c\u2500Embedding: 1-1                         [8, 1024, 768]            38,597,376\n",
       "\u251c\u2500Embedding: 1-2                         [1024, 768]               786,432\n",
       "\u251c\u2500Sequential: 1-3                        [8, 1024, 768]            --\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-1             [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-1               [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-2      [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-3               [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-4                     [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-2             [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-5               [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-6      [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-7               [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-8                     [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-3             [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-9               [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-10     [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-11              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-12                    [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-4             [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-13              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-14     [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-15              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-16                    [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-5             [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-17              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-18     [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-19              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-20                    [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-6             [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-21              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-22     [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-23              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-24                    [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-7             [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-25              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-26     [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-27              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-28                    [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-8             [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-29              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-30     [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-31              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-32                    [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-9             [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-33              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-34     [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-35              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-36                    [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-10            [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-37              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-38     [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-39              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-40                    [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-11            [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-41              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-42     [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-43              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-44                    [8, 1024, 768]            4,722,432\n",
       "\u2502    \u2514\u2500TransformerBlock: 2-12            [8, 1024, 768]            --\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-45              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MultiHeadAttention: 3-46     [8, 1024, 768]            2,359,296\n",
       "\u2502    \u2502    \u2514\u2500LayerNorm: 3-47              [8, 1024, 768]            1,536\n",
       "\u2502    \u2502    \u2514\u2500MLP: 3-48                    [8, 1024, 768]            4,722,432\n",
       "\u251c\u2500LayerNorm: 1-4                         [8, 1024, 768]            1,536\n",
       "\u251c\u2500Linear: 1-5                            [8, 1024, 50257]          38,597,376\n",
       "==========================================================================================\n",
       "Total params: 163,000,320\n",
       "Trainable params: 163,000,320\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.10\n",
       "==========================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 10044.38\n",
       "Params size (MB): 652.00\n",
       "Estimated Total Size (MB): 10696.44\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adc7e5cb",
    "outputId": "5b0f7b58-2a99-4634-b8bc-c8ffae7d2489"
   },
   "source": [
    "summary(gpt2, input_size=(batch_size, context_window_size), dtypes=[torch.long])"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "GPT2LMHeadModel                                    --                        --\n",
       "\u251c\u2500GPT2Model: 1-1                                   --                        --\n",
       "\u2502    \u2514\u2500Embedding: 2-1                              [8, 1024, 768]            38,597,376\n",
       "\u2502    \u2514\u2500Embedding: 2-2                              [1, 1024, 768]            786,432\n",
       "\u2502    \u2514\u2500Dropout: 2-3                                [8, 1024, 768]            --\n",
       "\u2502    \u2514\u2500ModuleList: 2-4                             --                        --\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-1                         [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-2                         [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-3                         [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-4                         [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-5                         [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-6                         [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-7                         [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-8                         [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-9                         [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-10                        [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-11                        [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2502    \u2514\u2500GPT2Block: 3-12                        [8, 1024, 768]            7,087,872\n",
       "\u2502    \u2514\u2500LayerNorm: 2-5                              [8, 1024, 768]            1,536\n",
       "\u251c\u2500Linear: 1-2                                      [8, 1024, 50257]          38,597,376\n",
       "====================================================================================================\n",
       "Total params: 163,037,184\n",
       "Trainable params: 163,037,184\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.TERABYTES): 1.31\n",
       "====================================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 10044.38\n",
       "Params size (MB): 652.15\n",
       "Estimated Total Size (MB): 10696.59\n",
       "===================================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  }
 ]
}