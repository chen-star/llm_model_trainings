{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPuaZT0d6++hmXLAjyvcr+B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chen-star/llm_model_trainings/blob/main/4_1_pre_training_weight_init.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> ‚≠ê Pre-training ‚≠ê"
      ],
      "metadata": {
        "id": "KC3IAwL1CNSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "YjQGM6suDL_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üöÜ Pre-train the model built in [notebook link](https://github.com/chen-star/llm_model_trainings/blob/main/3_4_transformer_impl_full_transformer_decoder.ipynb):"
      ],
      "metadata": {
        "id": "u-JVYcmACYuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weight initialization**\n",
        "\n",
        "* Why do weights need to be initialized?\n",
        "~~~text\n",
        "-> Small weights (those ~= 0) increase risk of vanishing gradients.\n",
        "\n",
        "-> Large weights increase risk of exploding gradients.\n",
        "~~~"
      ],
      "metadata": {
        "id": "TqlnbXyteXLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "7v-__TgiDIHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úà Imports"
      ],
      "metadata": {
        "id": "NjCzDw64C9ZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pyjeQceCB8yu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "from typing import override"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî¢ Hyperparameters"
      ],
      "metadata": {
        "id": "qYpGaBZRFkuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOIcFdRYDWmk",
        "outputId": "83ec2909-ed58-422d-bfb6-ffa009f78341"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same parameters a GPT2-124M\n",
        "batch_size = 8\n",
        "\n",
        "num_transformer_blocks = 12\n",
        "\n",
        "embedding_dimension = 768\n",
        "num_heads = 12 # embedding_dimension must be divisible by num_heads\n",
        "\n",
        "context_window_size = 1024\n",
        "vocabulary_size = 50257"
      ],
      "metadata": {
        "id": "-QqtTyr0Fovn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÇ Prepare Train / Test Data"
      ],
      "metadata": {
        "id": "a3rvftJ-JTah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import GPT2 tokenizer\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "2-kHHKkPJmHI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download << Romeo and Juliet >>\n",
        "text = requests.get('https://www.gutenberg.org/cache/epub/1513/pg1513.txt').text\n",
        "token_ids = torch.tensor( tokenizer.encode(text),dtype=torch.long )\n",
        "print(f'Token size: {len(token_ids)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ0b9DRgJqfF",
        "outputId": "f32a1881-8cae-4d88-ca1e-7b305ca66939"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (56185 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token size: 56185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split train / test data\n",
        "train_ratio = 0.85\n",
        "\n",
        "train_size = int(train_ratio * len(token_ids))\n",
        "train_data = token_ids[:train_size]\n",
        "test_data = token_ids[train_size:]"
      ],
      "metadata": {
        "id": "iM0jw_1KKkbg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a func to get data batches\n",
        "def get_batch(data, batch_size, context_window_size):\n",
        "  # generate random start idx\n",
        "  random_indices = torch.randint(0, len(data) - context_window_size, (batch_size,))\n",
        "\n",
        "  # X and targets\n",
        "  X = data[random_indices[:, None] + torch.arange(context_window_size)]\n",
        "  Y = data[random_indices[:, None] + torch.arange(1, context_window_size + 1)]\n",
        "\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "vGmpeJsILOYf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_batch():\n",
        "  return get_batch(train_data, batch_size, context_window_size)\n",
        "\n",
        "def get_test_batch():\n",
        "  return get_batch(test_data, batch_size, context_window_size)"
      ],
      "metadata": {
        "id": "dLD0JOYML_yv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [0] üèö Model\n",
        "\n",
        "The model built in [notebook link](https://github.com/chen-star/llm_model_trainings/blob/main/3_4_transformer_impl_full_transformer_decoder.ipynb)."
      ],
      "metadata": {
        "id": "Z5fgWDoiFv_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (0.1) üëì Multi-head Attention"
      ],
      "metadata": {
        "id": "xUoUbzSEGwWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedding_dimension, num_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    # define W_Q, W_K, W_V\n",
        "    self.q_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "    self.k_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "    self.v_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "    # define W0\n",
        "    self.w0_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "    # ***** multi-head *****\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dimension = embedding_dimension // num_heads\n",
        "    # *****************************\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X):\n",
        "    batch_size, context_window_size, embedding_dimension = X.shape\n",
        "\n",
        "    # Q = XW_Q\n",
        "    # K = XW_K\n",
        "    # V = XW_V\n",
        "    Q = self.q_layer(X)\n",
        "    K = self.k_layer(X)\n",
        "    V = self.v_layer(X)\n",
        "\n",
        "    # ***** Split Q,K,V *****\n",
        "    Q = Q.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
        "    K = K.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
        "    V = V.view(batch_size, context_window_size, self.num_heads, self.head_dimension)\n",
        "\n",
        "    # For attention score calculation, pytorch expects the shape to be\n",
        "    # [batch_size, num_heads, context_window_size, head_dimension]\n",
        "    Q = Q.transpose(1,2)\n",
        "    K = K.transpose(1,2)\n",
        "    V = V.transpose(1,2)\n",
        "    # *****************************\n",
        "\n",
        "    attention_score = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
        "\n",
        "    # Transpose back\n",
        "    attention_score = attention_score.transpose(1,2)\n",
        "\n",
        "    # ***** Merge heads *****\n",
        "    attention_score = attention_score.reshape(batch_size, context_window_size, embedding_dimension)\n",
        "    # *****************************\n",
        "\n",
        "    return self.w0_layer(attention_score)"
      ],
      "metadata": {
        "id": "dcu61J-AFuH9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (0.2) üèÉ Single MLP"
      ],
      "metadata": {
        "id": "xAgw4gf2G5xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, embedding_dimension, expansion: int=4):\n",
        "    super().__init__()\n",
        "\n",
        "    # define W1, Gelu, W2\n",
        "    self.w1_layer = nn.Linear(embedding_dimension, expansion * embedding_dimension) # 4x expansion\n",
        "    self.gelu = nn.GELU()\n",
        "    self.w2_layer = nn.Linear(expansion * embedding_dimension, embedding_dimension) # 4x contraction\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X):\n",
        "    W1 = self.w1_layer(X)\n",
        "    GELU = self.gelu(W1)\n",
        "    W2 = self.w2_layer(GELU)\n",
        "\n",
        "    return W2"
      ],
      "metadata": {
        "id": "A7wqS17bG3qH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (0.3) üî≤ Transformer Block"
      ],
      "metadata": {
        "id": "zLDXc5KtHA-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embedding_dimension):\n",
        "    super().__init__()\n",
        "\n",
        "    # Attention\n",
        "    self.layerNorm_attention = nn.LayerNorm(embedding_dimension)\n",
        "    self.attention_heads = MultiHeadAttention(embedding_dimension, num_heads)\n",
        "\n",
        "    # MLP / FeedForward\n",
        "    self.layerNorm_mlp = nn.LayerNorm(embedding_dimension)\n",
        "    self.mlp = MLP(embedding_dimension)\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, X):\n",
        "    # --- Attention ---\n",
        "    # X -> layerNorm -> attention_head\n",
        "    #                                     +   = output\n",
        "    #                                X\n",
        "    X = X + self.attention_heads(self.layerNorm_attention(X))\n",
        "\n",
        "    # --- MLP ---\n",
        "    # X -> layerNorm -> mlp\n",
        "    #                         +   = output\n",
        "    #                     X\n",
        "    X = X + self.mlp(self.layerNorm_mlp(X))\n",
        "\n",
        "    return X\n",
        ""
      ],
      "metadata": {
        "id": "n3K1--WlHAd-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] ‚úè Weight Init"
      ],
      "metadata": {
        "id": "_Kjk8E8wHrhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1.1) üîñ Define Model"
      ],
      "metadata": {
        "id": "rkfbSd6tM6qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    # ----- Token Embedding + Position Encoding -----\n",
        "    self.wte = nn.Embedding(vocabulary_size, embedding_dimension) # token embedding\n",
        "    self.wpe = nn.Embedding(context_window_size, embedding_dimension) # position encoding\n",
        "\n",
        "    # ----- Transformer Blocks -----\n",
        "    self.transformer_blocks = nn.Sequential(*[\n",
        "          TransformerBlock(embedding_dimension) for _ in range(num_transformer_blocks)\n",
        "        ])\n",
        "\n",
        "    # ----- Final layernorm -----\n",
        "    self.final_layernorm = nn.LayerNorm(embedding_dimension)\n",
        "\n",
        "    # ----- Unembedding -----\n",
        "    self.unembedding = nn.Linear(embedding_dimension, vocabulary_size, bias=False)\n",
        "    # tied unembedding weights\n",
        "    self.unembedding.weight = nn.Parameter(self.wte.weight)\n",
        "\n",
        "\n",
        "    # ------ Weight Init -----\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    # If current layer is linear, init with normal distribution\n",
        "    if isinstance(module, nn.Linear):\n",
        "      nn.init.normal_(module.weight, mean=0, std=0.02)\n",
        "      # If current layer has biases, init biases to zero\n",
        "      if module.bias is not None:\n",
        "        nn.init.zeros_(module.bias)\n",
        "\n",
        "    if isinstance(module, nn.Embedding):\n",
        "      nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "\n",
        "  @override\n",
        "  def forward(self, token_ids):\n",
        "    # ----- Token Embedding + Position Encoding -----\n",
        "    # [batch_size, context_window_size, embedding_dimension]\n",
        "    token_embedding = self.wte(token_ids)\n",
        "    # [context_window_size, embedding_dimension]\n",
        "    position_emcoding = self.wpe(torch.arange(token_ids.shape[-1], device=self.device))\n",
        "    # [batch_size, context_window_size, embedding_dimension]\n",
        "    X = token_embedding + position_emcoding\n",
        "\n",
        "    # ----- Transformer Blocks -----\n",
        "    X = self.transformer_blocks(X)\n",
        "\n",
        "    # ----- Final layernorm -----\n",
        "    X = self.final_layernorm(X)\n",
        "\n",
        "    # ----- Unembedding -----\n",
        "    # [batch_size, context_window_size, vocab_size]\n",
        "    logits = self.unembedding(X)\n",
        "\n",
        "    return logits\n",
        "  def generate(self, token_ids, temperature=1.1, num_new_tokens=10):\n",
        "    for _ in range(0, num_new_tokens):\n",
        "      # forward\n",
        "      # [batch_size, context_window_size, vocab_size]\n",
        "      logits = self(token_ids[:, -context_window_size:])\n",
        "      # [batch_size, vocab_size]\n",
        "      logits = logits[:, -1, :] # last token's logits\n",
        "\n",
        "      # softmax\n",
        "      # [batch_size, vocab_size]\n",
        "      probabilities = F.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "      # sample\n",
        "      # [batch_size, 1]\n",
        "      next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
        "\n",
        "      # append\n",
        "      token_ids = torch.cat((token_ids, next_token_id), dim=1)\n",
        "\n",
        "      return token_ids"
      ],
      "metadata": {
        "id": "CmW-scfQHGQo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1.2) üñ≤ Train Model"
      ],
      "metadata": {
        "id": "3tHQ1rXfNFgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model instance\n",
        "model = LanguageModel(device).to(device)"
      ],
      "metadata": {
        "id": "rkyg1_dbMTP8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the loss and optimizer functions\n",
        "loss_function = nn.NLLLoss().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=.001, weight_decay=.01)"
      ],
      "metadata": {
        "id": "WnBXCLw1NbIa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init losses\n",
        "train_loss = []\n",
        "test_loss = []"
      ],
      "metadata": {
        "id": "44rEwV6PNurh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = 666 + 1\n",
        "\n",
        "for i in range(sample_size):\n",
        "  X, targets = get_train_batch()\n",
        "  X = X.to(device)\n",
        "  targets = targets.to(device)\n",
        "\n",
        "  # clear previous gradients\n",
        "  model.zero_grad(set_to_none=True)\n",
        "\n",
        "  # forward\n",
        "  log_probs = model(X)\n",
        "\n",
        "  # calc losses\n",
        "  loss = loss_function(\n",
        "      log_probs.view(-1, log_probs.shape[-1]),\n",
        "      targets.view(-1)\n",
        "  )\n",
        "\n",
        "  # back propagate\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # save loss\n",
        "  train_loss.append(loss.item())\n",
        "\n",
        "  # during training, test on test data\n",
        "  if i%111 == 0: # test 6 times\n",
        "    with torch.no_grad():\n",
        "      X, targets = get_test_batch()\n",
        "      X = X.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # forward\n",
        "      output = model(X)\n",
        "\n",
        "      # calc losses\n",
        "      curr_test_loss = loss_function(\n",
        "          output.view(-1, output.shape[-1]),\n",
        "          targets.view(-1)\n",
        "      )\n",
        "\n",
        "      # save loss\n",
        "      test_loss.append(curr_test_loss.item())\n",
        "      print(f\"test the [{i//111}th] data, current test loss is: {test_loss[-1]}\")\n",
        "\n",
        "print(f\"mean train loss: {np.mean(train_loss)}\")\n",
        "print(f\"mean test loss: {np.mean(test_loss)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTbRr6sQO9HM",
        "outputId": "b88e4bdf-aaf7-4b1a-fe2b-f4d9510c3ccb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test the [0th] data, current test loss is: -1.3252832889556885\n",
            "test the [1th] data, current test loss is: -78.15104675292969\n",
            "test the [2th] data, current test loss is: -226.57412719726562\n",
            "test the [3th] data, current test loss is: -435.824462890625\n",
            "test the [4th] data, current test loss is: -715.3743896484375\n",
            "test the [5th] data, current test loss is: -1019.90283203125\n",
            "test the [6th] data, current test loss is: -1383.8614501953125\n",
            "mean train loss: -643.4822124206479\n",
            "mean test loss: -551.5733702863965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the losses\n",
        "plt.plot(train_loss,'k',label='Train loss')\n",
        "plt.plot(range(0, sample_size, 111),test_loss,'rs-',markerfacecolor='w',markersize=8,label='Test loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.gca().set(xlabel='Epoch',ylabel='Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "O9Wbqx21Rd_0",
        "outputId": "a1bbaa20-3dc2-4931-f954-c3dbe915d866"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"426.027812pt\" height=\"310.86825pt\" viewBox=\"0 0 426.027812 310.86825\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-11-29T07:01:30.467358</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 310.86825 \nL 426.027812 310.86825 \nL 426.027812 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 61.707813 273.312 \nL 418.827813 273.312 \nL 418.827813 7.2 \nL 61.707813 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"ma8670d05dc\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma8670d05dc\" x=\"77.94054\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(74.75929 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#ma8670d05dc\" x=\"126.687469\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(117.143719 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#ma8670d05dc\" x=\"175.434397\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(165.890647 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#ma8670d05dc\" x=\"224.181326\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(214.637576 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#ma8670d05dc\" x=\"272.928255\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(263.384505 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#ma8670d05dc\" x=\"321.675184\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(312.131434 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#ma8670d05dc\" x=\"370.422112\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(360.878362 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- Epoch -->\n     <g transform=\"translate(224.956875 301.588562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-45\"/>\n      <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(63.183594 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(126.660156 0)\"/>\n      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(187.841797 0)\"/>\n      <use xlink:href=\"#DejaVuSans-68\" transform=\"translate(242.822266 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path id=\"me00bd9e7ee\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#me00bd9e7ee\" x=\"61.707813\" y=\"260.593966\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- ‚àí1750 -->\n      <g transform=\"translate(20.878125 264.393185) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(83.789062 0)\"/>\n       <use xlink:href=\"#DejaVuSans-37\" transform=\"translate(147.412109 0)\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(211.035156 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(274.658203 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#me00bd9e7ee\" x=\"61.707813\" y=\"226.121644\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- ‚àí1500 -->\n      <g transform=\"translate(20.878125 229.920863) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(83.789062 0)\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(147.412109 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(211.035156 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(274.658203 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#me00bd9e7ee\" x=\"61.707813\" y=\"191.649322\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- ‚àí1250 -->\n      <g transform=\"translate(20.878125 195.448541) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(83.789062 0)\"/>\n       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(147.412109 0)\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(211.035156 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(274.658203 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#me00bd9e7ee\" x=\"61.707813\" y=\"157.177001\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- ‚àí1000 -->\n      <g transform=\"translate(20.878125 160.976219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(83.789062 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(147.412109 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(211.035156 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(274.658203 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#me00bd9e7ee\" x=\"61.707813\" y=\"122.704679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- ‚àí750 -->\n      <g transform=\"translate(27.240625 126.503897) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-37\" transform=\"translate(83.789062 0)\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(147.412109 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(211.035156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#me00bd9e7ee\" x=\"61.707813\" y=\"88.232357\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- ‚àí500 -->\n      <g transform=\"translate(27.240625 92.031575) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(83.789062 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(147.412109 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(211.035156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#me00bd9e7ee\" x=\"61.707813\" y=\"53.760035\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- ‚àí250 -->\n      <g transform=\"translate(27.240625 57.559253) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(83.789062 0)\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(147.412109 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(211.035156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#me00bd9e7ee\" x=\"61.707813\" y=\"19.287713\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0 -->\n      <g transform=\"translate(48.345313 23.086931) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- Loss -->\n     <g transform=\"translate(14.798438 151.223187) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(53.962891 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(115.144531 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(167.244141 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 77.94054 19.296 \nL 78.915478 19.662698 \nL 80.865355 19.949397 \nL 81.352825 19.35165 \nL 81.840294 20.102989 \nL 85.252579 20.615377 \nL 89.152333 21.388954 \nL 90.614741 21.655913 \nL 92.077149 22.006236 \nL 93.052088 22.195541 \nL 107.676166 25.802387 \nL 108.163636 26.006953 \nL 108.651105 25.996427 \nL 110.600982 26.563483 \nL 121.812776 29.946528 \nL 122.300245 30.199664 \nL 122.787714 30.235273 \nL 123.275184 30.483712 \nL 123.762653 30.569255 \nL 124.250122 30.839343 \nL 125.225061 31.061774 \nL 125.71253 31.295865 \nL 126.199999 31.334802 \nL 127.662407 31.951548 \nL 128.637346 32.136442 \nL 129.612284 32.476081 \nL 130.099754 32.78183 \nL 130.587223 32.827345 \nL 131.562161 33.220864 \nL 132.049631 33.34184 \nL 133.024569 33.762281 \nL 133.512039 33.724999 \nL 134.974446 34.671075 \nL 135.949385 34.613986 \nL 136.436854 35.224353 \nL 136.924324 35.05985 \nL 137.411793 35.312497 \nL 137.899262 35.341292 \nL 138.386731 35.60177 \nL 139.36167 36.296633 \nL 139.849139 36.198833 \nL 140.336609 36.33517 \nL 140.824078 36.721429 \nL 141.311547 36.719279 \nL 141.799016 36.896096 \nL 142.286486 37.296256 \nL 143.261424 37.699219 \nL 143.748894 37.725843 \nL 144.723832 38.2968 \nL 145.211301 38.249141 \nL 146.18624 38.794942 \nL 146.673709 38.961418 \nL 147.161179 38.875185 \nL 147.648648 39.140395 \nL 148.136117 39.631432 \nL 148.623586 39.793926 \nL 149.598525 39.878678 \nL 150.573464 40.506523 \nL 151.060933 40.79486 \nL 151.548402 40.616582 \nL 152.035871 41.279211 \nL 154.473218 42.340264 \nL 154.960687 42.311026 \nL 155.448156 42.574412 \nL 155.935626 42.411563 \nL 156.910564 43.402987 \nL 159.347911 44.139454 \nL 160.322849 44.892025 \nL 160.810319 44.825719 \nL 161.785257 45.344887 \nL 162.760196 45.66202 \nL 163.247665 45.956813 \nL 163.735134 46.565294 \nL 164.222604 46.399598 \nL 164.710073 46.919031 \nL 165.197542 46.072303 \nL 165.685012 47.203683 \nL 166.172481 47.567544 \nL 166.65995 47.693649 \nL 167.147419 48.149778 \nL 167.634889 48.210782 \nL 168.122358 48.103796 \nL 168.609827 48.611354 \nL 169.097297 48.528872 \nL 169.584766 48.965882 \nL 170.072235 49.241703 \nL 170.559704 49.188132 \nL 171.047174 50.06325 \nL 171.534643 49.799669 \nL 172.022112 50.202051 \nL 172.509582 50.173548 \nL 173.48452 50.974829 \nL 174.459459 51.138392 \nL 174.946928 51.662453 \nL 175.434397 51.849994 \nL 175.921867 52.161437 \nL 176.896805 52.48822 \nL 177.871744 52.969166 \nL 178.359213 53.544225 \nL 178.846682 53.400411 \nL 179.334152 53.908046 \nL 180.30909 54.561186 \nL 180.796559 54.566179 \nL 181.771498 55.719638 \nL 182.258967 55.601425 \nL 182.746437 55.667201 \nL 183.233906 56.058007 \nL 183.721375 56.049035 \nL 185.671252 56.923022 \nL 186.158722 57.837876 \nL 186.646191 57.638844 \nL 187.13366 58.15353 \nL 187.621129 57.829065 \nL 188.108599 58.4432 \nL 188.596068 58.774937 \nL 189.083537 59.297143 \nL 189.571007 58.869353 \nL 190.058476 59.361038 \nL 190.545945 60.100451 \nL 192.008353 60.925274 \nL 192.983292 61.287987 \nL 193.95823 61.360332 \nL 194.445699 61.699841 \nL 195.420638 62.920011 \nL 195.908107 63.303864 \nL 196.883046 63.162163 \nL 197.370515 63.918151 \nL 197.857984 63.710905 \nL 198.345454 64.697258 \nL 199.320392 64.744729 \nL 199.807862 65.332063 \nL 200.295331 64.946547 \nL 200.7828 65.508936 \nL 203.220147 67.45129 \nL 203.707616 67.206433 \nL 204.195085 67.797709 \nL 205.170024 67.868064 \nL 206.144962 68.768868 \nL 206.632432 69.762017 \nL 207.60737 69.444722 \nL 208.582309 70.817682 \nL 209.069778 70.410553 \nL 209.557247 71.266748 \nL 210.044717 71.302234 \nL 210.532186 71.639463 \nL 211.019655 72.436265 \nL 211.507125 72.713096 \nL 211.994594 72.598456 \nL 212.482063 72.759393 \nL 212.969532 73.772593 \nL 213.457002 73.164585 \nL 213.944471 73.585251 \nL 214.43194 73.482638 \nL 214.91941 74.961379 \nL 215.406879 74.269997 \nL 215.894348 75.072354 \nL 216.381817 75.295094 \nL 217.356756 76.550473 \nL 217.844225 76.768769 \nL 218.331695 76.252021 \nL 218.819164 77.342932 \nL 219.306633 76.168071 \nL 219.794102 77.443997 \nL 220.281572 78.363283 \nL 220.769041 78.626656 \nL 221.25651 79.06739 \nL 221.74398 79.27977 \nL 222.231449 79.270319 \nL 222.718918 80.189078 \nL 223.206387 79.793169 \nL 223.693857 80.586419 \nL 224.181326 79.993699 \nL 225.156265 81.066532 \nL 225.643734 82.090939 \nL 226.131203 82.100643 \nL 226.618672 81.039171 \nL 227.106142 81.738608 \nL 227.593611 83.269634 \nL 228.08108 83.899162 \nL 228.56855 83.942446 \nL 229.056019 84.448657 \nL 229.543488 85.211837 \nL 230.030957 84.977411 \nL 230.518427 85.697143 \nL 231.005896 85.40191 \nL 231.980835 86.110726 \nL 232.468304 86.233951 \nL 232.955773 86.859641 \nL 233.443242 86.999954 \nL 233.930712 87.649597 \nL 234.418181 87.873061 \nL 235.39312 87.959069 \nL 235.880589 88.776684 \nL 236.368058 89.192633 \nL 236.855527 89.005627 \nL 237.342997 90.17979 \nL 237.830466 90.911485 \nL 238.317935 91.137845 \nL 238.805405 91.023924 \nL 239.292874 91.863497 \nL 239.780343 91.109752 \nL 240.267813 91.918 \nL 240.755282 91.87496 \nL 241.242751 93.470483 \nL 241.73022 93.373319 \nL 242.21769 93.57608 \nL 242.705159 93.560805 \nL 243.680098 94.550731 \nL 244.167567 94.740101 \nL 244.655036 94.407607 \nL 245.142505 95.732022 \nL 245.629975 95.510713 \nL 246.117444 96.003424 \nL 246.604913 97.725526 \nL 247.092383 96.806438 \nL 247.579852 97.434194 \nL 248.067321 97.804031 \nL 248.55479 97.209484 \nL 249.04226 98.062565 \nL 249.529729 99.491618 \nL 250.504668 99.832663 \nL 250.992137 99.385668 \nL 251.479606 99.771554 \nL 252.942014 102.650776 \nL 253.429483 102.770991 \nL 253.916953 102.22716 \nL 254.891891 103.391425 \nL 255.37936 103.8261 \nL 255.86683 104.633128 \nL 256.354299 103.952223 \nL 256.841768 105.529602 \nL 257.329238 105.758654 \nL 258.304176 105.910859 \nL 258.791645 106.775798 \nL 259.279115 106.519629 \nL 259.766584 107.060119 \nL 260.254053 107.88514 \nL 260.741523 107.833061 \nL 261.228992 108.690518 \nL 261.716461 108.647461 \nL 262.20393 109.380672 \nL 262.6914 109.630697 \nL 263.178869 110.658782 \nL 263.666338 109.437859 \nL 264.153808 111.036319 \nL 264.641277 111.425059 \nL 265.616215 110.679755 \nL 266.103685 112.822029 \nL 266.591154 113.5224 \nL 267.078623 112.634241 \nL 268.053562 114.230951 \nL 268.541031 113.565684 \nL 269.0285 115.385942 \nL 270.003439 116.511199 \nL 270.490908 115.3037 \nL 270.978378 115.382475 \nL 271.465847 116.660054 \nL 271.953316 116.205434 \nL 272.440785 118.392827 \nL 272.928255 117.439587 \nL 273.415724 119.079294 \nL 273.903193 117.672948 \nL 274.390663 120.136617 \nL 274.878132 118.7782 \nL 275.85307 121.316671 \nL 276.828009 121.427065 \nL 277.315478 122.374633 \nL 277.802948 122.37199 \nL 278.290417 122.566234 \nL 278.777886 123.302406 \nL 279.265355 123.760789 \nL 279.752825 123.639505 \nL 280.240294 125.19963 \nL 280.727763 125.275619 \nL 281.215233 125.85664 \nL 281.702702 125.959914 \nL 282.190171 126.334927 \nL 282.677641 126.446314 \nL 283.16511 127.690103 \nL 283.652579 127.424845 \nL 284.140048 128.554201 \nL 284.627518 128.201777 \nL 285.602456 129.893605 \nL 286.089926 129.824475 \nL 286.577395 130.363821 \nL 287.064864 130.306962 \nL 287.552333 131.07477 \nL 288.039803 131.590467 \nL 288.527272 131.326513 \nL 289.014741 131.749901 \nL 289.502211 132.413561 \nL 289.98968 134.194027 \nL 290.477149 134.310052 \nL 290.964618 134.144339 \nL 291.452088 134.94109 \nL 291.939557 134.575916 \nL 292.427026 136.255785 \nL 292.914496 136.671885 \nL 293.401965 135.91155 \nL 293.889434 137.569437 \nL 294.864373 137.951342 \nL 295.351842 137.255112 \nL 295.839311 139.27649 \nL 296.326781 137.743431 \nL 296.81425 140.204145 \nL 297.301719 139.477828 \nL 297.789188 140.793651 \nL 298.276658 141.306856 \nL 298.764127 140.817595 \nL 299.251596 141.581027 \nL 299.739066 143.209911 \nL 300.226535 142.090133 \nL 300.714004 141.520608 \nL 301.201473 142.9299 \nL 301.688943 142.479083 \nL 302.176412 145.007328 \nL 302.663881 143.498306 \nL 303.151351 145.832249 \nL 303.63882 145.782703 \nL 305.101228 147.546262 \nL 305.588697 147.527023 \nL 306.563636 149.030718 \nL 307.538574 149.557784 \nL 308.026043 150.656 \nL 308.513513 150.409156 \nL 309.000982 151.729246 \nL 309.488451 153.566664 \nL 309.975921 152.52997 \nL 310.46339 152.826646 \nL 310.950859 153.431637 \nL 311.438328 153.501726 \nL 311.925798 154.531578 \nL 312.413267 156.015832 \nL 312.900736 156.767347 \nL 313.388206 154.531418 \nL 313.875675 155.196887 \nL 314.363144 157.333834 \nL 314.850613 156.889406 \nL 315.338083 156.609823 \nL 315.825552 159.566414 \nL 316.313021 159.585485 \nL 316.800491 158.916388 \nL 317.28796 160.853723 \nL 317.775429 161.080436 \nL 318.750368 162.335696 \nL 319.237837 162.045408 \nL 319.725306 160.708309 \nL 320.212776 163.704035 \nL 320.700245 164.409337 \nL 321.187714 162.411407 \nL 321.675184 163.494575 \nL 322.162653 165.371045 \nL 322.650122 165.17468 \nL 323.137591 167.083821 \nL 323.625061 165.710516 \nL 324.11253 164.986362 \nL 324.599999 167.436186 \nL 325.087469 168.091025 \nL 325.574938 166.598986 \nL 326.062407 167.644905 \nL 326.549876 169.800637 \nL 327.037346 169.887592 \nL 327.524815 170.481903 \nL 328.012284 170.423983 \nL 328.499754 172.732619 \nL 328.987223 171.739503 \nL 329.474692 169.149231 \nL 329.962161 173.685641 \nL 330.449631 174.497928 \nL 330.9371 174.950815 \nL 331.424569 172.405418 \nL 331.912039 176.330432 \nL 332.399508 176.288941 \nL 332.886977 176.504544 \nL 333.374446 177.009863 \nL 333.861916 177.764669 \nL 334.349385 178.184565 \nL 334.836854 179.676789 \nL 335.324324 177.253728 \nL 335.811793 179.187864 \nL 336.299262 178.609275 \nL 336.786731 181.337352 \nL 337.274201 179.883135 \nL 337.76167 181.772497 \nL 338.249139 181.326326 \nL 338.736609 183.004748 \nL 339.711547 183.007694 \nL 340.199016 183.088606 \nL 341.173955 186.695172 \nL 341.661424 184.748597 \nL 342.148894 186.497629 \nL 342.636363 186.525975 \nL 343.123832 186.442134 \nL 343.611301 189.16984 \nL 344.098771 188.429527 \nL 344.58624 189.434038 \nL 345.561179 189.023164 \nL 346.048648 190.320152 \nL 347.023586 191.860045 \nL 347.511056 191.44333 \nL 347.998525 191.510642 \nL 348.485994 194.084334 \nL 348.973464 194.178527 \nL 349.460933 192.821853 \nL 349.948402 196.219514 \nL 350.435871 196.478646 \nL 350.923341 195.504079 \nL 351.41081 197.560922 \nL 351.898279 197.325035 \nL 352.385749 198.153213 \nL 352.873218 196.651883 \nL 353.360687 201.11524 \nL 353.848156 199.503457 \nL 354.335626 198.961545 \nL 354.823095 202.302617 \nL 355.310564 200.475348 \nL 355.798034 203.035423 \nL 356.285503 202.464626 \nL 356.772972 202.276257 \nL 357.260441 203.365872 \nL 357.747911 202.522647 \nL 358.23538 203.990851 \nL 358.722849 203.673262 \nL 359.210319 204.24463 \nL 359.697788 206.897939 \nL 360.185257 205.528455 \nL 360.672727 205.705614 \nL 361.160196 205.60149 \nL 361.647665 209.020478 \nL 362.135134 209.587823 \nL 362.622604 211.542057 \nL 363.110073 207.802029 \nL 363.597542 210.969443 \nL 364.085012 212.743118 \nL 364.572481 210.595869 \nL 365.05995 213.618291 \nL 365.547419 211.545609 \nL 366.034889 213.46298 \nL 366.522358 214.08732 \nL 367.984766 216.788566 \nL 368.472235 215.565674 \nL 369.447174 218.856838 \nL 369.934643 219.368486 \nL 370.422112 220.136792 \nL 370.909582 219.028325 \nL 371.397051 219.948759 \nL 371.88452 220.18786 \nL 372.371989 224.203246 \nL 372.859459 221.945982 \nL 373.346928 223.619523 \nL 373.834397 224.596951 \nL 374.321867 221.818984 \nL 374.809336 222.27246 \nL 375.296805 224.067023 \nL 375.784274 228.439184 \nL 376.271744 226.556874 \nL 376.759213 225.905166 \nL 377.246682 227.602709 \nL 377.734152 231.659266 \nL 378.221621 228.753222 \nL 378.70909 230.02476 \nL 379.196559 229.244655 \nL 379.684029 232.410773 \nL 380.171498 230.829018 \nL 380.658967 231.781939 \nL 381.146437 229.669802 \nL 381.633906 234.002324 \nL 382.121375 231.958761 \nL 382.608844 234.656086 \nL 383.096314 234.81781 \nL 383.583783 232.931881 \nL 384.071252 236.129895 \nL 384.558722 235.139304 \nL 385.046191 236.896888 \nL 385.53366 236.722001 \nL 386.021129 236.19581 \nL 386.508599 239.816515 \nL 386.996068 239.925487 \nL 387.483537 240.675697 \nL 387.971007 238.158039 \nL 388.945945 244.21963 \nL 389.433414 243.865296 \nL 389.920884 242.859657 \nL 390.408353 242.646578 \nL 390.895822 244.122004 \nL 391.870761 245.695948 \nL 392.35823 247.531313 \nL 392.845699 246.230319 \nL 393.333169 248.662907 \nL 393.820638 248.960904 \nL 394.308107 248.406132 \nL 394.795577 249.162756 \nL 395.283046 250.357421 \nL 395.770515 251.016636 \nL 396.745454 251.535236 \nL 397.232923 250.101369 \nL 397.720392 255.063044 \nL 398.207862 253.264491 \nL 398.695331 255.016116 \nL 399.1828 255.906606 \nL 399.67027 257.44394 \nL 400.157739 257.524667 \nL 400.645208 257.171545 \nL 401.620147 259.23709 \nL 402.107616 255.689235 \nL 402.595085 261.216 \nL 402.595085 261.216 \n\" clip-path=\"url(#pa3eb75cd5a)\" style=\"fill: none; stroke: #000000; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 77.94054 19.470455 \nL 132.049631 30.063905 \nL 186.158722 50.529858 \nL 240.267813 79.383237 \nL 294.376903 117.930178 \nL 348.485994 159.921388 \nL 402.595085 210.107383 \n\" clip-path=\"url(#pa3eb75cd5a)\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n    <defs>\n     <path id=\"m2ea778f2a8\" d=\"M -4 4 \nL 4 4 \nL 4 -4 \nL -4 -4 \nz\n\" style=\"stroke: #ff0000; stroke-linejoin: miter\"/>\n    </defs>\n    <g clip-path=\"url(#pa3eb75cd5a)\">\n     <use xlink:href=\"#m2ea778f2a8\" x=\"77.94054\" y=\"19.470455\" style=\"fill: #ffffff; stroke: #ff0000; stroke-linejoin: miter\"/>\n     <use xlink:href=\"#m2ea778f2a8\" x=\"132.049631\" y=\"30.063905\" style=\"fill: #ffffff; stroke: #ff0000; stroke-linejoin: miter\"/>\n     <use xlink:href=\"#m2ea778f2a8\" x=\"186.158722\" y=\"50.529858\" style=\"fill: #ffffff; stroke: #ff0000; stroke-linejoin: miter\"/>\n     <use xlink:href=\"#m2ea778f2a8\" x=\"240.267813\" y=\"79.383237\" style=\"fill: #ffffff; stroke: #ff0000; stroke-linejoin: miter\"/>\n     <use xlink:href=\"#m2ea778f2a8\" x=\"294.376903\" y=\"117.930178\" style=\"fill: #ffffff; stroke: #ff0000; stroke-linejoin: miter\"/>\n     <use xlink:href=\"#m2ea778f2a8\" x=\"348.485994\" y=\"159.921388\" style=\"fill: #ffffff; stroke: #ff0000; stroke-linejoin: miter\"/>\n     <use xlink:href=\"#m2ea778f2a8\" x=\"402.595085\" y=\"210.107383\" style=\"fill: #ffffff; stroke: #ff0000; stroke-linejoin: miter\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 61.707813 273.312 \nL 61.707813 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 418.827813 273.312 \nL 418.827813 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 61.707813 273.312 \nL 418.827813 273.312 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 61.707813 7.2 \nL 418.827813 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 333.346563 44.55625 \nL 411.827813 44.55625 \nQ 413.827813 44.55625 413.827813 42.55625 \nL 413.827813 14.2 \nQ 413.827813 12.2 411.827813 12.2 \nL 333.346563 12.2 \nQ 331.346563 12.2 331.346563 14.2 \nL 331.346563 42.55625 \nQ 331.346563 44.55625 333.346563 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 335.346563 20.298437 \nL 345.346563 20.298437 \nL 355.346563 20.298437 \n\" style=\"fill: none; stroke: #000000; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- Train loss -->\n     <g transform=\"translate(363.346563 23.798437) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(46.333984 0)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(87.447266 0)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(148.726562 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(176.509766 0)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(239.888672 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(271.675781 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(299.458984 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(360.640625 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(412.740234 0)\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 335.346563 34.976562 \nL 345.346563 34.976562 \nL 355.346563 34.976562 \n\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n     <g>\n      <use xlink:href=\"#m2ea778f2a8\" x=\"345.346563\" y=\"34.976562\" style=\"fill: #ffffff; stroke: #ff0000; stroke-linejoin: miter\"/>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- Test loss -->\n     <g transform=\"translate(363.346563 38.476562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(44.083984 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(105.607422 0)\"/>\n      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(157.707031 0)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(196.916016 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(228.703125 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(256.486328 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(317.667969 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(369.767578 0)\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa3eb75cd5a\">\n   <rect x=\"61.707813\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}