{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO6BPDbfSMw7QQQvwmhBMml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chen-star/llm_model_trainings/blob/main/7_2_evaluation_quantitative_HellaSwag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> \u2b50 Evaluation \u2b50"
      ],
      "metadata": {
        "id": "KC3IAwL1CNSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "YjQGM6suDL_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Goal:** To test a LLM's ability to perform *commonsense reasoning*.\n",
        "\n",
        "* **How it works?**\n",
        "\n",
        "The core task is sentence completion. Models are given a context, typically a short description of an everyday situation or activity, and then presented with four possible endings. The model must select the most plausible and commonsense ending from the options."
      ],
      "metadata": {
        "id": "5tq8tzGfsAxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "7v-__TgiDIHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \u2708 Imports"
      ],
      "metadata": {
        "id": "NjCzDw64C9ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  !pip install bitsandbytes"
      ],
      "metadata": {
        "id": "CpmUOWZHAcWP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pyjeQceCB8yu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \ud83d\udd22 Hyperparameters"
      ],
      "metadata": {
        "id": "qYpGaBZRFkuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOIcFdRYDWmk",
        "outputId": "cdfb38e2-6b80-45ac-83f1-354dd63670f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_window_size = 1024"
      ],
      "metadata": {
        "id": "FWNekrtzYX4w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \ud83d\udc36 HellaSwag"
      ],
      "metadata": {
        "id": "bOGzCgUqRNGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# func to test one HellaSwag data sample\n",
        "def test_one_sample(model, sample, tokenizer):\n",
        "  ctx = sample['ctx']\n",
        "  log_prob_sum_per_option = np.zeros(len(sample['endings']))\n",
        "\n",
        "  for option in range(len(sample['endings'])):\n",
        "    prompt = f\"{ctx} {sample['endings'][option]}\"\n",
        "    prompt_tokens = tokenizer.encode(prompt, return_tensors = 'pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(prompt_tokens).logits\n",
        "\n",
        "    log_probs = F.log_softmax(logits, dim = -1)\n",
        "\n",
        "    # sum of log_probs of predicated tokens\n",
        "    log_prob_sum = np.array([\n",
        "        log_probs[0, i, prompt_tokens[0][i+1]].item()\n",
        "        for i in range(0, len(prompt_tokens[0])-1)\n",
        "    ])\n",
        "    log_prob_sum_per_option[option] = log_prob_sum.sum()\n",
        "\n",
        "  # return the log_prob for each option, and the true answer\n",
        "  return log_prob_sum_per_option, int(sample['label'])"
      ],
      "metadata": {
        "id": "Mbc6R-7kBmHG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare the HellaSwag results for different models"
      ],
      "metadata": {
        "id": "4xMXLKtHEBlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Zephyr Model"
      ],
      "metadata": {
        "id": "ZpN2IZomEsXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import zephyr model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True, # preserve 4-bit precision\n",
        "    bnb_4bit_compute_dtype = 'float16',\n",
        "    bnb_4bit_use_double_quant = True,\n",
        ")\n",
        "\n",
        "zephyr_model = AutoModelForCausalLM.from_pretrained('HuggingFaceH4/zephyr-7b-alpha',\n",
        "    quantization_config = quantization_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "50b7035347c149a1aa28aa6a791d8c0d",
            "de1c826f9e4a47fc84ef76ff3a0dd489",
            "46c5370c9e1843d1a9c0fd651f68acde",
            "744994dbf0d94fcab060b9788c4c0c3e",
            "aba82aade8f74ddfa50916126b6b2f35",
            "4f83eb695cac4912ac27d1fe98f32975",
            "bcfa2fe005b548cb93660c3c5626b212",
            "9135f387539f413eab43569f636943d8",
            "9ff2611db49b4381bb4464454434c9c2",
            "d0eeaf75b05e46d79dcc23b036c59274",
            "91e6bf1e4d2e45e391dc74634f374b02"
          ]
        },
        "id": "qs1QCdpv47P3",
        "outputId": "c8dca4b7-9dd2-44f1-e56a-d332af575a59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50b7035347c149a1aa28aa6a791d8c0d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zephyr_model.eval()\n",
        "zephyr_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdGFU2vs5LBJ",
        "outputId": "37b3b0af-9db5-43bf-f90a-cefe92c946ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MistralDecoderLayer(\n",
              "        (self_attn): MistralAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): MistralRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer\n",
        "zephyr_tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-alpha')"
      ],
      "metadata": {
        "id": "Zaah1bxSBFWJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import GPT-2 Model"
      ],
      "metadata": {
        "id": "VWwyskAoEwtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import GPT2 and disable normalizations\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2').to(device)"
      ],
      "metadata": {
        "id": "uHXGobZUEbx3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN7vVVsYEfvI",
        "outputId": "b8733e85-fd35-483e-8b41-473c65e0398b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "Uha6iyFlEnEB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run test for both models"
      ],
      "metadata": {
        "id": "OGaWyCceEzF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('hellaswag',split='validation')"
      ],
      "metadata": {
        "id": "gMBSfz75FLE7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = 666\n",
        "results = np.zeros((2, sample_size))\n",
        "\n",
        "def is_correct(log_prob_sum_per_option, true_answer):\n",
        "  return 1 if np.argmax(log_prob_sum_per_option) == true_answer else 0\n",
        "\n",
        "for i in tqdm(range(sample_size), desc='HellaSwag Evaluation'):\n",
        "  one_sample = dataset[i]\n",
        "\n",
        "  # zephyr model\n",
        "  log_prob_sum_per_option, true_answer = test_one_sample(zephyr_model, one_sample, zephyr_tokenizer)\n",
        "  results[0, i] = is_correct(log_prob_sum_per_option, true_answer)\n",
        "\n",
        "  # gpt-2 model\n",
        "  log_prob_sum_per_option, true_answer = test_one_sample(gpt2_model, one_sample, gpt2_tokenizer)\n",
        "  results[1, i] = is_correct(log_prob_sum_per_option, true_answer)\n",
        "\n",
        "print(f\"Zephyr Accuracy: {np.mean(results[0])}\")\n",
        "print(f\"GPT-2 Accuracy: {np.mean(results[1])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5mpTcUDEyZ8",
        "outputId": "f6857056-2b8b-45e0-a33f-b6b26f3756ef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "HellaSwag Evaluation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 666/666 [09:35<00:00,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zephyr Accuracy: 0.5285285285285285\n",
            "GPT-2 Accuracy: 0.35585585585585583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}