{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chen-star/llm_model_trainings/blob/main/impl_byte_pair_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5ac7e9",
      "metadata": {
        "id": "ba5ac7e9"
      },
      "source": [
        "# Byte Pair Encoding (BPE)\n",
        "\n",
        "Byte Pair Encoding (BPE) is a subword tokenization method widely used in Natural Language Processing (NLP), particularly in Large Language Models (LLMs) like GPT, RoBERTa, and others.\n",
        "\n",
        "## How it Works\n",
        "\n",
        "1.  **Initialization**: Start with a vocabulary of individual characters.\n",
        "2.  **Counting**: Count the frequency of all adjacent pairs of symbols in the text.\n",
        "3.  **Merging**: Identify the most frequent pair and merge them into a new symbol.\n",
        "4.  **Iteration**: Repeat steps 2 and 3 for a fixed number of merges (hyperparameter) or until a desired vocabulary size is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c75491e8",
      "metadata": {
        "id": "c75491e8"
      },
      "source": [
        "## Example Walkthrough\n",
        "\n",
        "Let's assume we have a small corpus with the following word frequencies:\n",
        "\n",
        "*   \"low\": 5\n",
        "*   \"lower\": 2\n",
        "*   \"newest\": 6\n",
        "*   \"widest\": 3\n",
        "\n",
        "### Step 1: Initialization\n",
        "\n",
        "We split words into characters and append a special end-of-word symbol `</w>`.\n",
        "\n",
        "*   `l o w </w>`: 5\n",
        "*   `l o w e r </w>`: 2\n",
        "*   `n e w e s t </w>`: 6\n",
        "*   `w i d e s t </w>`: 3\n",
        "\n",
        "**Vocabulary**: `l, o, w, e, r, n, s, t, i, d, </w>`\n",
        "\n",
        "### Step 2: Counting Pairs\n",
        "\n",
        "We count the frequency of all adjacent pairs.\n",
        "\n",
        "*   `e s`: 6 (newest) + 3 (widest) = **9**\n",
        "*   `s t`: 6 (newest) + 3 (widest) = **9**\n",
        "*   `e s t`: (overlaps, but we count bigrams first)\n",
        "*   `l o`: 5 (low) + 2 (lower) = 7\n",
        "*   `o w`: 5 (low) + 2 (lower) = 7\n",
        "*   ...\n",
        "\n",
        "### Step 3: Merging\n",
        "\n",
        "The most frequent pair is `e` and `s` (9 times) or `s` and `t` (9 times). Let's pick `e` and `s` to merge into `es`.\n",
        "\n",
        "*   `l o w </w>`: 5\n",
        "*   `l o w e r </w>`: 2\n",
        "*   `n e w es t </w>`: 6\n",
        "*   `w i d es t </w>`: 3\n",
        "\n",
        "**New Token**: `es`\n",
        "\n",
        "### Step 4: Iteration\n",
        "\n",
        "Now we count pairs again. The pair `es` and `t` appears 9 times (6 in newest + 3 in widest).\n",
        "\n",
        "Merge `es` and `t` -> `est`.\n",
        "\n",
        "*   `l o w </w>`: 5\n",
        "*   `l o w e r </w>`: 2\n",
        "*   `n e w est </w>`: 6\n",
        "*   `w i d est </w>`: 3\n",
        "\n",
        "**New Token**: `est`\n",
        "\n",
        "We continue this process until we reach a desired vocabulary size."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] ðŸ—º Implement BPE"
      ],
      "metadata": {
        "id": "P71NJSC1ca8t"
      },
      "id": "P71NJSC1ca8t"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "5yT0iLzC0dce"
      },
      "id": "5yT0iLzC0dce",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_pair_freq(text: str) -> dict[str, int]:\n",
        "  pair_counts = {}\n",
        "\n",
        "  for i in range(len(text) - 1):\n",
        "    pair = text[i] + text[i + 1]\n",
        "    pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
        "\n",
        "  return pair_counts"
      ],
      "metadata": {
        "id": "L7EPDuvYcd0F"
      },
      "id": "L7EPDuvYcd0F",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_most_frequent_pair(pair_counts: dict[str, int]) -> str:\n",
        "  maxIdx = np.argmax(list(pair_counts.values()))\n",
        "  return list(pair_counts.keys())[maxIdx]"
      ],
      "metadata": {
        "id": "5TG7s0CG0tNf"
      },
      "id": "5TG7s0CG0tNf",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_vocabulary(vocabulary: dict[str, int], pair: str) -> dict[str, int]:\n",
        "    vocabulary[pair] = max(vocabulary.values()) + 1\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "5pmrM7Pb03CE"
      },
      "id": "5pmrM7Pb03CE",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_merged_text(text: str, pair: str) -> str:\n",
        "  merged_text = []\n",
        "\n",
        "  i = 0\n",
        "  while i < (len(text) - 1):\n",
        "    curr_pair = text[i] + text[i + 1]\n",
        "\n",
        "    if curr_pair == pair:\n",
        "      merged_text.append(curr_pair)\n",
        "      i += 2\n",
        "    else:\n",
        "      merged_text.append(text[i])\n",
        "      i += 1\n",
        "\n",
        "  if i == (len(text) - 1):\n",
        "    merged_text.append(text[i])\n",
        "\n",
        "  return merged_text"
      ],
      "metadata": {
        "id": "G2Ix3Slu2SDj"
      },
      "id": "G2Ix3Slu2SDj",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bpe(text: str, target_vocab_size=30) -> dict[str, int]:\n",
        "  # initialize each char in text to vacabulary\n",
        "  chars = list(set(text))\n",
        "  chars.sort()\n",
        "  vocabulary = {c : i for i, c in enumerate(chars)}\n",
        "\n",
        "  # run bpe\n",
        "  updated_text = text\n",
        "  while len(vocabulary) < target_vocab_size:\n",
        "    pair_counts = count_pair_freq(updated_text)\n",
        "    max_pair = select_most_frequent_pair(pair_counts)\n",
        "    vocabulary = update_vocabulary(vocabulary, max_pair)\n",
        "    updated_text = generate_merged_text(updated_text, max_pair)\n",
        "    print(f\"*** vocab_size={len(vocabulary)}:\")\n",
        "    print(f\"\\t {vocabulary}\")\n",
        "\n",
        "  return vocabulary"
      ],
      "metadata": {
        "id": "AaPKngUi3GVr"
      },
      "id": "AaPKngUi3GVr",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] ðŸ§ª Test BPE"
      ],
      "metadata": {
        "id": "uDQThtUw4pyk"
      },
      "id": "uDQThtUw4pyk"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "This is a much more complex example text for byte pair encoding demonstration.\n",
        "We will observe how it tokenizes common words and subword units effectively.\n",
        "\"\"\"\n",
        "print(f\"Original text: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KX7dm154nLy",
        "outputId": "d708c224-8762-47cb-a26d-7f81abaeb945"
      },
      "id": "7KX7dm154nLy",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: \n",
            "This is a much more complex example text for byte pair encoding demonstration. \n",
            "We will observe how it tokenizes common words and subword units effectively.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_vocabulary = bpe(text, target_vocab_size=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmIwGbkh5c7A",
        "outputId": "97e52cf8-a99c-4600-f000-071b5c0f6e8d"
      },
      "id": "gmIwGbkh5c7A",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** vocab_size=30:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29}\n",
            "*** vocab_size=31:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29, 'e ': 30}\n",
            "*** vocab_size=32:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29, 'e ': 30, 'or': 31}\n",
            "*** vocab_size=33:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29, 'e ': 30, 'or': 31, 'co': 32}\n",
            "*** vocab_size=34:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29, 'e ': 30, 'or': 31, 'co': 32, 'ex': 33}\n",
            "*** vocab_size=35:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29, 'e ': 30, 'or': 31, 'co': 32, 'ex': 33, 'on': 34}\n",
            "*** vocab_size=36:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29, 'e ': 30, 'or': 31, 'co': 32, 'ex': 33, 'on': 34, 'is ': 35}\n",
            "*** vocab_size=37:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29, 'e ': 30, 'or': 31, 'co': 32, 'ex': 33, 'on': 34, 'is ': 35, ' m': 36}\n",
            "*** vocab_size=38:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29, 'e ': 30, 'or': 31, 'co': 32, 'ex': 33, 'on': 34, 'is ': 35, ' m': 36, 'com': 37}\n",
            "*** vocab_size=39:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29, 'e ': 30, 'or': 31, 'co': 32, 'ex': 33, 'on': 34, 'is ': 35, ' m': 36, 'com': 37, 'pl': 38}\n",
            "*** vocab_size=40:\n",
            "\t {'\\n': 0, ' ': 1, '.': 2, 'T': 3, 'W': 4, 'a': 5, 'b': 6, 'c': 7, 'd': 8, 'e': 9, 'f': 10, 'g': 11, 'h': 12, 'i': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, 's ': 29, 'e ': 30, 'or': 31, 'co': 32, 'ex': 33, 'on': 34, 'is ': 35, ' m': 36, 'com': 37, 'pl': 38, 't ': 39}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}